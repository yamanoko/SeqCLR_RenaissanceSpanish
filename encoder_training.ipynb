{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-27T01:30:35.926977Z",
     "start_time": "2024-03-27T01:30:34.517327Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import torch.utils.data\n",
    "from torch.utils.data import DataLoader, ConcatDataset\n",
    "\n",
    "from encoder import Encoder\n",
    "from custom_dataset import ContrastiveLearningDataset\n",
    "from custom_loss import contrastive_loss\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "config = json.load(open('config.json', 'r'))[\"SSL\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f039402b58fd4a5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-27T01:30:36.781092Z",
     "start_time": "2024-03-27T01:30:36.383272Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = Encoder()\n",
    "model = model.to(device)\n",
    "optimizer = Adam(model.parameters(), lr=config[\"start lr\"])\n",
    "scheduler = StepLR(optimizer, step_size=config[\"lr scheduler step size\"], gamma=0.1)\n",
    "dataset = []\n",
    "for i in range(3):\n",
    "\tif config[f\"dataset {i}\"] is not None:\n",
    "\t\tdataset.append(ContrastiveLearningDataset(img_dir=config[f\"dataset {i}\"]))\n",
    "dataset = ConcatDataset(dataset)\n",
    "train_dataloader = DataLoader(dataset, batch_size=config[\"Batch size\"], shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "64d7d878a9df3900",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-27T01:56:43.402986Z",
     "start_time": "2024-03-27T01:30:50.232616Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Batch 1/2481, Loss:407.0021667480469\n",
      "Epoch 1/5, Batch 101/2481, Loss:350.645751953125\n",
      "Epoch 1/5, Batch 201/2481, Loss:349.701171875\n",
      "Epoch 1/5, Batch 301/2481, Loss:349.75555419921875\n",
      "Epoch 1/5, Batch 401/2481, Loss:348.7194519042969\n",
      "Epoch 1/5, Batch 501/2481, Loss:348.22564697265625\n",
      "Epoch 1/5, Batch 601/2481, Loss:347.7273254394531\n",
      "Epoch 1/5, Batch 701/2481, Loss:347.35858154296875\n",
      "Epoch 1/5, Batch 801/2481, Loss:347.50958251953125\n",
      "Epoch 1/5, Batch 901/2481, Loss:347.58740234375\n",
      "Epoch 1/5, Batch 1001/2481, Loss:347.45751953125\n",
      "Epoch 1/5, Batch 1101/2481, Loss:348.71856689453125\n",
      "Epoch 1/5, Batch 1201/2481, Loss:347.76416015625\n",
      "Epoch 1/5, Batch 1301/2481, Loss:347.580322265625\n",
      "Epoch 1/5, Batch 1401/2481, Loss:347.369140625\n",
      "Epoch 1/5, Batch 1501/2481, Loss:347.216552734375\n",
      "Epoch 1/5, Batch 1601/2481, Loss:347.2215576171875\n",
      "Epoch 1/5, Batch 1701/2481, Loss:347.15191650390625\n",
      "Epoch 1/5, Batch 1801/2481, Loss:347.2649230957031\n",
      "Epoch 1/5, Batch 1901/2481, Loss:347.0948181152344\n",
      "Epoch 1/5, Batch 2001/2481, Loss:347.04534912109375\n",
      "Epoch 1/5, Batch 2101/2481, Loss:346.9912109375\n",
      "Epoch 1/5, Batch 2201/2481, Loss:346.9503173828125\n",
      "Epoch 1/5, Batch 2301/2481, Loss:346.92547607421875\n",
      "Epoch 1/5, Batch 2401/2481, Loss:346.90966796875\n",
      "Epoch 1/5, Average Loss:348.228654516267\n",
      "Epoch 2/5, Batch 1/2481, Loss:346.89447021484375\n",
      "Epoch 2/5, Batch 101/2481, Loss:346.8674011230469\n",
      "Epoch 2/5, Batch 201/2481, Loss:346.86041259765625\n",
      "Epoch 2/5, Batch 301/2481, Loss:346.8425598144531\n",
      "Epoch 2/5, Batch 401/2481, Loss:346.8279113769531\n",
      "Epoch 2/5, Batch 501/2481, Loss:346.82061767578125\n",
      "Epoch 2/5, Batch 601/2481, Loss:346.8154296875\n",
      "Epoch 2/5, Batch 701/2481, Loss:346.81097412109375\n",
      "Epoch 2/5, Batch 801/2481, Loss:346.8060302734375\n",
      "Epoch 2/5, Batch 901/2481, Loss:346.804443359375\n",
      "Epoch 2/5, Batch 1001/2481, Loss:346.8018493652344\n",
      "Epoch 2/5, Batch 1101/2481, Loss:346.8013916015625\n",
      "Epoch 2/5, Batch 1201/2481, Loss:346.8016052246094\n",
      "Epoch 2/5, Batch 1301/2481, Loss:346.8020935058594\n",
      "Epoch 2/5, Batch 1401/2481, Loss:346.79937744140625\n",
      "Epoch 2/5, Batch 1501/2481, Loss:346.7996826171875\n",
      "Epoch 2/5, Batch 1601/2481, Loss:346.798095703125\n",
      "Epoch 2/5, Batch 1701/2481, Loss:346.7998046875\n",
      "Epoch 2/5, Batch 1801/2481, Loss:346.79779052734375\n",
      "Epoch 2/5, Batch 1901/2481, Loss:346.7978515625\n",
      "Epoch 2/5, Batch 2001/2481, Loss:346.79766845703125\n",
      "Epoch 2/5, Batch 2101/2481, Loss:346.7980651855469\n",
      "Epoch 2/5, Batch 2201/2481, Loss:346.7975769042969\n",
      "Epoch 2/5, Batch 2301/2481, Loss:346.79779052734375\n",
      "Epoch 2/5, Batch 2401/2481, Loss:346.79742431640625\n",
      "Epoch 2/5, Average Loss:346.715086958861\n",
      "Epoch 3/5, Batch 1/2481, Loss:346.797607421875\n",
      "Epoch 3/5, Batch 101/2481, Loss:346.7971496582031\n",
      "Epoch 3/5, Batch 201/2481, Loss:346.7978515625\n",
      "Epoch 3/5, Batch 301/2481, Loss:346.7973937988281\n",
      "Epoch 3/5, Batch 401/2481, Loss:346.7969970703125\n",
      "Epoch 3/5, Batch 501/2481, Loss:346.797119140625\n",
      "Epoch 3/5, Batch 601/2481, Loss:346.79669189453125\n",
      "Epoch 3/5, Batch 701/2481, Loss:346.7969665527344\n",
      "Epoch 3/5, Batch 801/2481, Loss:346.7967834472656\n",
      "Epoch 3/5, Batch 901/2481, Loss:346.79669189453125\n",
      "Epoch 3/5, Batch 1001/2481, Loss:346.7970275878906\n",
      "Epoch 3/5, Batch 1101/2481, Loss:346.7969665527344\n",
      "Epoch 3/5, Batch 1201/2481, Loss:346.7969970703125\n",
      "Epoch 3/5, Batch 1301/2481, Loss:346.7966003417969\n",
      "Epoch 3/5, Batch 1401/2481, Loss:346.797119140625\n",
      "Epoch 3/5, Batch 1501/2481, Loss:346.79681396484375\n",
      "Epoch 3/5, Batch 1601/2481, Loss:346.80413818359375\n",
      "Epoch 3/5, Batch 1701/2481, Loss:346.807373046875\n",
      "Epoch 3/5, Batch 1801/2481, Loss:346.80072021484375\n",
      "Epoch 3/5, Batch 1901/2481, Loss:346.796875\n",
      "Epoch 3/5, Batch 2001/2481, Loss:346.7977600097656\n",
      "Epoch 3/5, Batch 2101/2481, Loss:346.7967224121094\n",
      "Epoch 3/5, Batch 2201/2481, Loss:346.7969970703125\n",
      "Epoch 3/5, Batch 2301/2481, Loss:346.7967834472656\n",
      "Epoch 3/5, Batch 2401/2481, Loss:346.79718017578125\n",
      "Epoch 3/5, Average Loss:346.7029338253933\n",
      "Epoch 4/5, Batch 1/2481, Loss:346.7969970703125\n",
      "Epoch 4/5, Batch 101/2481, Loss:346.79766845703125\n",
      "Epoch 4/5, Batch 201/2481, Loss:346.7969970703125\n",
      "Epoch 4/5, Batch 301/2481, Loss:346.7968444824219\n",
      "Epoch 4/5, Batch 401/2481, Loss:346.79766845703125\n",
      "Epoch 4/5, Batch 501/2481, Loss:346.7970275878906\n",
      "Epoch 4/5, Batch 601/2481, Loss:346.797119140625\n",
      "Epoch 4/5, Batch 701/2481, Loss:346.79669189453125\n",
      "Epoch 4/5, Batch 801/2481, Loss:346.7966003417969\n",
      "Epoch 4/5, Batch 901/2481, Loss:346.796630859375\n",
      "Epoch 4/5, Batch 1001/2481, Loss:346.796630859375\n",
      "Epoch 4/5, Batch 1101/2481, Loss:346.79669189453125\n",
      "Epoch 4/5, Batch 1201/2481, Loss:346.796875\n",
      "Epoch 4/5, Batch 1301/2481, Loss:346.7968444824219\n",
      "Epoch 4/5, Batch 1401/2481, Loss:346.79669189453125\n",
      "Epoch 4/5, Batch 1501/2481, Loss:346.796630859375\n",
      "Epoch 4/5, Batch 1601/2481, Loss:346.79656982421875\n",
      "Epoch 4/5, Batch 1701/2481, Loss:346.796630859375\n",
      "Epoch 4/5, Batch 1801/2481, Loss:346.7964172363281\n",
      "Epoch 4/5, Batch 1901/2481, Loss:346.796630859375\n",
      "Epoch 4/5, Batch 2001/2481, Loss:346.7965393066406\n",
      "Epoch 4/5, Batch 2101/2481, Loss:346.7965087890625\n",
      "Epoch 4/5, Batch 2201/2481, Loss:346.79656982421875\n",
      "Epoch 4/5, Batch 2301/2481, Loss:346.7967834472656\n",
      "Epoch 4/5, Batch 2401/2481, Loss:346.7965087890625\n",
      "Epoch 4/5, Average Loss:346.70013360696566\n",
      "Epoch 5/5, Batch 1/2481, Loss:346.8005065917969\n",
      "Epoch 5/5, Batch 101/2481, Loss:346.796875\n",
      "Epoch 5/5, Batch 201/2481, Loss:346.79681396484375\n",
      "Epoch 5/5, Batch 301/2481, Loss:346.7965087890625\n",
      "Epoch 5/5, Batch 401/2481, Loss:346.7965087890625\n",
      "Epoch 5/5, Batch 501/2481, Loss:346.7967529296875\n",
      "Epoch 5/5, Batch 601/2481, Loss:346.7965087890625\n",
      "Epoch 5/5, Batch 701/2481, Loss:346.7965087890625\n",
      "Epoch 5/5, Batch 801/2481, Loss:346.7965087890625\n",
      "Epoch 5/5, Batch 901/2481, Loss:346.79644775390625\n",
      "Epoch 5/5, Batch 1001/2481, Loss:346.7965393066406\n",
      "Epoch 5/5, Batch 1101/2481, Loss:346.79644775390625\n",
      "Epoch 5/5, Batch 1201/2481, Loss:346.7965087890625\n",
      "Epoch 5/5, Batch 1301/2481, Loss:346.7964172363281\n",
      "Epoch 5/5, Batch 1401/2481, Loss:346.7965087890625\n",
      "Epoch 5/5, Batch 1501/2481, Loss:346.79730224609375\n",
      "Epoch 5/5, Batch 1601/2481, Loss:346.7965087890625\n",
      "Epoch 5/5, Batch 1701/2481, Loss:346.79644775390625\n",
      "Epoch 5/5, Batch 1801/2481, Loss:346.7964782714844\n",
      "Epoch 5/5, Batch 1901/2481, Loss:346.7965087890625\n",
      "Epoch 5/5, Batch 2001/2481, Loss:346.79644775390625\n",
      "Epoch 5/5, Batch 2101/2481, Loss:346.7965393066406\n",
      "Epoch 5/5, Batch 2201/2481, Loss:346.79644775390625\n",
      "Epoch 5/5, Batch 2301/2481, Loss:346.79644775390625\n",
      "Epoch 5/5, Batch 2401/2481, Loss:346.79638671875\n",
      "Epoch 5/5, Average Loss:346.69996494230173\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "epochs = config[\"epoch size\"]\n",
    "step = 0\n",
    "steps = []\n",
    "loss_list = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    batch_loss = 0\n",
    "    for i, batch in enumerate(train_dataloader):\n",
    "        original, augmented = batch['original'], batch['augmented']\n",
    "        original = original.to(device)\n",
    "        augmented = augmented.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        original_embeddings, _ = model(original)\n",
    "        augmented_embeddings, _ = model(augmented)\n",
    "        \n",
    "        avg_pool = nn.AdaptiveAvgPool2d((original_embeddings.shape[1] // 4, original_embeddings.shape[2]))\n",
    "\n",
    "        original_embeddings = avg_pool(original_embeddings)\n",
    "        augmented_embeddings = avg_pool(augmented_embeddings)\n",
    "\n",
    "        flattened_original = original_embeddings.reshape(original_embeddings.shape[0] * original_embeddings.shape[1], -1)\n",
    "        flattened_augmented = augmented_embeddings.reshape(original_embeddings.shape[0] * original_embeddings.shape[1], -1)\n",
    "\n",
    "        loss = contrastive_loss(flattened_original, flattened_augmented)\n",
    "        batch_loss += loss.item()\n",
    "        if i % 100 == 0:\n",
    "            print(f\"Epoch {epoch + 1}/{epochs}, Batch {i + 1}/{len(train_dataloader)}, Loss:{loss.item()}\")\n",
    "            step += 100\n",
    "            steps.append(step)\n",
    "            loss_list.append(loss.item())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}, Average Loss:{batch_loss / len(train_dataloader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "260fc71c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.plot(steps, loss_list, xlabel=\"Steps\", ylabel=\"Loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8f9b334fe656ec77",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-27T02:00:56.866874Z",
     "start_time": "2024-03-27T02:00:56.550725Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), config[\"saved Encoder path\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
